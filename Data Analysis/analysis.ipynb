{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IRIS Data Analysis - Structural Schema & Relationship Mapping\n",
                "\n",
                "This notebook focuses on identifying the database schema, Primary Keys (PK), and Foreign Keys (FK) to determine table relationships and isolate irrelevant (junk) files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import glob\n",
                "import os\n",
                "import itertools\n",
                "\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Clean Data\n",
                "Loading all CSVs and dropping the `index` column immediately."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = 'Sales Dataset'\n",
                "csv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n",
                "dataframes = {}\n",
                "\n",
                "for file in csv_files:\n",
                "    filename = os.path.basename(file)\n",
                "    try:\n",
                "        try:\n",
                "            df = pd.read_csv(file, encoding='utf-8')\n",
                "        except UnicodeDecodeError:\n",
                "            df = pd.read_csv(file, encoding='ISO-8859-1')\n",
                "            \n",
                "        # Drop 'index' column if it exists\n",
                "        if 'index' in df.columns:\n",
                "            df = df.drop(columns=['index'])\n",
                "            \n",
                "        # Standardize column names (strip whitespace, lower case for comparison)\n",
                "        # But keep original for display\n",
                "        df.columns = [c.strip() for c in df.columns]\n",
                "            \n",
                "        dataframes[filename] = df\n",
                "        print(f\"Loaded {filename}: {df.shape}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {filename}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Identify Primary Keys (PK)\n",
                "A Primary Key must be unique and non-null. We will check each column in every dataframe."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "potential_pks = {}\n",
                "\n",
                "print(\"--- Potential Primary Keys ---\")\n",
                "for name, df in dataframes.items():\n",
                "    pks = []\n",
                "    for col in df.columns:\n",
                "        if df[col].is_unique and not df[col].isnull().any():\n",
                "            pks.append(col)\n",
                "    potential_pks[name] = pks\n",
                "    print(f\"{name}: {pks}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Identify Relationships (Foreign Keys)\n",
                "We look for columns that share names and data content between tables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Relationship Mapping (Shared Columns) ---\")\n",
                "file_pairs = itertools.combinations(dataframes.keys(), 2)\n",
                "connections = []\n",
                "\n",
                "for name1, name2 in file_pairs:\n",
                "    cols1 = set(dataframes[name1].columns)\n",
                "    cols2 = set(dataframes[name2].columns)\n",
                "    \n",
                "    # Find common columns\n",
                "    common_cols = cols1.intersection(cols2)\n",
                "    \n",
                "    # Also check for fuzzy matches (e.g. 'SKU' vs 'Sku' vs 'SKU Code')\n",
                "    # We'll normalize to lower case for this check\n",
                "    cols1_lower = {c.lower(): c for c in cols1}\n",
                "    cols2_lower = {c.lower(): c for c in cols2}\n",
                "    common_lower = set(cols1_lower.keys()).intersection(set(cols2_lower.keys()))\n",
                "    \n",
                "    for c_lower in common_lower:\n",
                "        c1 = cols1_lower[c_lower]\n",
                "        c2 = cols2_lower[c_lower]\n",
                "        \n",
                "        # Verify content overlap to confirm it's a real relationship\n",
                "        vals1 = set(dataframes[name1][c1].dropna().unique())\n",
                "        vals2 = set(dataframes[name2][c2].dropna().unique())\n",
                "        \n",
                "        overlap = vals1.intersection(vals2)\n",
                "        if len(overlap) > 0:\n",
                "            print(f\"{name1} ({c1}) <-> {name2} ({c2}) | Overlap: {len(overlap)}\")\n",
                "            connections.append((name1, name2))\n",
                "        else:\n",
                "             print(f\"{name1} ({c1}) <-> {name2} ({c2}) | No content overlap (False Positive)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Isolate Junk Tables\n",
                "Tables that have NO connections to others are likely junk or standalone reference files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "connected_files = set()\n",
                "for n1, n2 in connections:\n",
                "    connected_files.add(n1)\n",
                "    connected_files.add(n2)\n",
                "    \n",
                "all_files = set(dataframes.keys())\n",
                "junk_files = all_files - connected_files\n",
                "\n",
                "print(\"\\n--- Connected Files (Core Schema) ---\")\n",
                "for f in connected_files:\n",
                "    print(f)\n",
                "\n",
                "print(\"\\n--- Isolated Files (Potential Junk) ---\")\n",
                "for f in junk_files:\n",
                "    print(f)\n",
                "    print(f\"Preview of {f}:\")\n",
                "    display(dataframes[f].head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}